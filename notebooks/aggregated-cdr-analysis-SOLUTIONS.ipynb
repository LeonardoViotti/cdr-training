{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LeonardoViotti/cdr-training/blob/develop/notebooks/aggregated-cdr-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKd3pER2LVMu"
   },
   "source": [
    "# CDR Analysis practical exercises\n",
    "\n",
    "This notebook contains exercises to analyze aggregated CDR data. \n",
    "\n",
    "It uses mock CDR data for Ghana from February to May 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsOgtQfYalb"
   },
   "source": [
    "# Environment set-up\n",
    "Before we start, please run the cell below to set-up the environment. You can hide this section afterwards by clicking the arrow next to the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VOVB-FHL0yL"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Libraries installation\n",
    "\n",
    "!pip install geopandas\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# User defined functions\n",
    "\n",
    "def time_complete(data, timefreq = 'D'):\n",
    "    data = data.reset_index()\n",
    "    timevar = 'date'\n",
    "    data[timevar] = data[timevar].astype('datetime64[D]')\n",
    "    full_time_range = pd.date_range(data[timevar].min(),  \n",
    "                                            data[timevar].max(), \n",
    "                                            freq = timefreq)\n",
    "    data = data.set_index(timevar)\n",
    "    data = data.reindex(full_time_range,  fill_value=0)\n",
    "    data.index.name = 'date'\n",
    "    return(data)\n",
    "\n",
    "def day_lag(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by region and date\n",
    "    df = df.sort_values(['date'])\n",
    "    \n",
    "    # Lag value\n",
    "    df['value_l'] = df['value'].shift(1)\n",
    "    \n",
    "    # Drop values if missing dates\n",
    "    df['value_l'] = df['value_l'].where(df.date.diff() == dt.timedelta(days = 1), np.nan)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SAq88YZLwB"
   },
   "source": [
    "# Let's start\n",
    "First let's import the packages we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCRBfqMGLjq7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K86xN61OmPU"
   },
   "source": [
    "Now, import the datasets we will use on this exercise using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq68weiSOV-X"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if all files were loaded into Colab by running the cell below. You should see the following files:\n",
    " - admin1.geojson\n",
    " - movements_per_day.csv\n",
    " - subscribers_per_day.csv\n",
    " - transactions_per_day.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0FPdBvpOZQQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load and have a look at each one of the indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqCoflgQZRSY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load transactions per day data\n",
    "trans = pd.read_csv('transactions_per_day.csv')\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load subscribers per day data\n",
    "subs = pd.read_csv('subscribers_per_day.csv')\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movements\n",
    "mov = pd.read_csv('movements_per_day.csv')\n",
    "mov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgBRPdR0L_yK"
   },
   "source": [
    "## Exercise 1 - Quality checks\n",
    "\n",
    "First step is to take a quick look at the completeness and consistency of the data.\n",
    "\n",
    "- Check the number of regions per day\n",
    "- Check time completeness of the series\n",
    "- Compare number of subscribers to the number of calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Aggregate the transactions data by `date` using the `groupby()` and `agg()` methods. Store the result in a new DataFrame called `trans_day`\n",
    "    - Calculate the number of unique regions per day (TIP: use the `pd.Series.nunique` function.)\n",
    "    - Calculate sum of total transactions per day in the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Slice the `trans_day` DataFrame keeping only rows where there are fewer than 16 regions. Is there any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the time complete function (user defined) to create a new DataFrame replacing missing rows with zeros:\n",
    "    `trans_day_tcomplete = time_complete(trans_day)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the `value` column of the time complete data frame.\n",
    " - TIP: you can use the `plot()` method of the DataFrame, but pass `y = 'value'` a argument in order not to plot the `region` column also. \n",
    "\n",
    "Do you see anything unusual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPfN_10taird"
   },
   "outputs": [],
   "source": [
    "# 1. Aggregate data by day summing values across all regions\n",
    "trans_day = trans.groupby('date').agg({'region': pd.Series.nunique,\n",
    "                                       'value': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  Test if any date has fewer than 16 regions\n",
    "trans_day[trans_day['region'] < 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGI6n3rzaird",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLot\n",
    "trans_day_tcomplete = time_complete(trans_day)\n",
    "trans_day_tcomplete.plot(y = 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 2 instructions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Merge `subs` and `trans` using `['region', 'date']` columns as keys.\n",
    "\n",
    "    TIP: use the `suffixes` argument to differentiate the values on from each DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the `plot.scatter()` method on the merged DataFrame to compare the values of the two columns.\n",
    " - Set the x axis as the number of subscribers\n",
    " - Set the y axis as the number of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "mdf = subs.merge(trans, \n",
    "           on = ['region', 'date'], \n",
    "           suffixes = ('_trans', '_subs'))\n",
    "\n",
    "mdf.plot.scatter(x = 'value_subs',\n",
    "                 y = 'value_trans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVWvRSW0MHdY"
   },
   "source": [
    "## Exercise 2 - Changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSUj4uo9iCqT"
   },
   "source": [
    "Now let's look how movement has changed over time. \n",
    "\n",
    "For simplicity we will use country level data and only look at movements between two different regions. Here's a quick summary of the comparisons we'll do:\n",
    "\n",
    "- Absolute values\n",
    "- Change from previous day\n",
    "- Change from Baseline (defined as the average from February 1st to March 15th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here is a time-line to help interpret the results:\n",
    "- February 1st to March 15th: Baseline period\n",
    "- March 16th: initial restrictions imposed\n",
    "- March 30th: Lockdown measures on parts of Accra and Kumasi metropolitan areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On the `mov` DataFrame, remove rows where users move within a region.\n",
    " - TIP: slice the the DataFrame keeping only rows that match the condition `mov['region_from'] != mov['region_to']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aggregate `mov` DataFrame by `date` to have country level data using the `groupby()` and `agg()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the `day_lag()` function to create a DataFrame with a column containing the value of movements from the previous day.\n",
    "\n",
    "`mov_day = day_lag(mov_day)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the `bl_values()` function to create a DataFrame with a column containing the average number of movements in the baseline.\n",
    "\n",
    "`mov_day = bl_values(mov_day)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move to part 4, lets review what the `bl_value()` function is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl_values(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create weekday variable to calculate baseline values\n",
    "    df['weekday'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Keep only entries from Feb 1st to Mar 15th\n",
    "    bl = df[df['date'] < dt.datetime(2020, 3, 16)]\n",
    "\n",
    "    # Calculate baseline averages for each weekday\n",
    "    bl_averages = bl.groupby(['weekday']).agg({'value': np.mean}).reset_index()\n",
    "    \n",
    "    # Merge bl averages as a column on original df\n",
    "    ndf = df.merge(bl_averages, on = ['weekday'],\n",
    "                  suffixes = ('', '_bl')).drop('weekday', axis = 1)\n",
    "\n",
    "    return ndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create two columns containing:\n",
    " - Percent changes from previous day; and\n",
    " - Percent change from baseline columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create 3 different line plots:\n",
    "    - Level values of total movements\n",
    "    - Percent change from previous day\n",
    "    - Percent change from baseline\n",
    "\n",
    "TIP: You can again use the `plot()` method keeping sure to specify the x and y axis.\n",
    "\n",
    "How does these 3 plots compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove movements in the same district\n",
    "mov = mov[mov['region_from'] != mov['region_to']]\n",
    "\n",
    "# 2. Aggregate by day\n",
    "mov_day = mov.groupby('date').agg({'value' : np.sum}).reset_index()\n",
    "\n",
    "# 3\n",
    "mov_day = day_lag(mov_day)\n",
    "\n",
    "# 4\n",
    "mov_day = bl_values(mov_day)\n",
    "\n",
    "# # 5. Calculate percent change columns\n",
    "mov_day['p_change_l'] = (mov_day['value'] - mov_day['value_l'])/mov_day['value_l']\n",
    "\n",
    "mov_day['p_change_bl'] = (mov_day['value'] - mov_day['value_bl'])/mov_day['value_bl']\n",
    "\n",
    "mov_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mov_day.plot(x = 'date', y = 'value')\n",
    "ax.axvline('2020-03-16', color='orange', linestyle='--', lw=1)\n",
    "ax.axvline('2020-03-30', color='red', linestyle='--', lw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mov_day.plot(x = 'date', y = 'p_change_l')\n",
    "ax.axvline('2020-03-16', color='orange', linestyle='--', lw=1)\n",
    "ax.axvline('2020-03-30', color='red', linestyle='--', lw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mov_day.plot(x = 'date', y = 'p_change_bl')\n",
    "ax.axvline('2020-03-16', color='orange', linestyle='--', lw=1)\n",
    "ax.axvline('2020-03-30', color='red', linestyle='--', lw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yDEra3bMHoG"
   },
   "source": [
    "\n",
    "## Exercise 3 - Choropleth\n",
    "As a final exercise we will create a map using the `admin1.geojson` to display changes in the changes on the average number of daily trips between districts since baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start run the cell bellow for the initial set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI954J60amL3"
   },
   "outputs": [],
   "source": [
    "# Load choropleth package\n",
    "import folium\n",
    "\n",
    "# Load the geoson in two ways\n",
    "gdf = gpd.read_file(\"admin1.geojson\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the geometries data looks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the administrative boundaries\n",
    "gdf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a region level DataFrame containing pre and post baseline movement levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_region_df(df, region = 'region_from'):\n",
    "    \n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Keep only entries from Feb 1st to Mar 15th\n",
    "    bl = df[df['date'] < dt.datetime(2020, 3, 16)]\n",
    "    \n",
    "    # Post baseline\n",
    "    post = df[df['date'] >= dt.datetime(2020, 3, 16)]\n",
    "\n",
    "    # Calculate baseline \n",
    "    bl_movs = bl.groupby([region]).agg({'value': np.mean}).reset_index()\n",
    "    \n",
    "    # Calculate region df\n",
    "    rdf = post.groupby([region]).agg({'value': np.mean}).reset_index()\n",
    "\n",
    "    \n",
    "    # Merge bl averages as a column on original df\n",
    "    ndf = rdf.merge(bl_movs, on = [region],\n",
    "                  suffixes = ('_pos', '_bl'))\n",
    "    \n",
    "    # Percent change\n",
    "    ndf['p_change'] = (ndf['value_pos'] - ndf['value_bl'])/ndf['value_bl']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ndf.rename(columns = {region : 'region'})\n",
    "\n",
    "mov_region = create_region_df(mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, combine the GeoDataFrame and the `mov_region` DataFrame so our interest data and the geometries are in the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.merge(mov_region, on = 'region')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a base map in Ghana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load base map\n",
    "gmap = folium.Map(location=[7.28, -0.97], zoom_start=7)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets use `gdf` to add a layer to the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create choropleth on top of map\n",
    "choropleth = folium.Choropleth(\n",
    "    gdf,\n",
    "    data=gdf,\n",
    "    key_on=\"properties.region\",\n",
    "    columns=[\"region\", \"p_change\"],\n",
    "    fill_color=\"RdPu\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2).add_to(gmap)\n",
    "\n",
    "# Add hover information\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name', 'p_change'])\n",
    ")\n",
    "\n",
    "# Print\n",
    "gmap"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": " aggregated-cdr-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
