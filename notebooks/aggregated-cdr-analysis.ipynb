{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LeonardoViotti/cdr-training/blob/develop/notebooks/aggregated-cdr-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKd3pER2LVMu"
   },
   "source": [
    "# CDR Analysis\n",
    "\n",
    "This notebook contains exercises to analyze aggregated CDR data. It uses mock CDR data for Ghana from February to May 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsOgtQfYalb"
   },
   "source": [
    "# Environment set-up\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VOVB-FHL0yL"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Libraries installation\n",
    "\n",
    "!pip install geopandas\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# User defined functions\n",
    "\n",
    "def time_complete(data, timefreq = 'D'):\n",
    "    data = data.reset_index()\n",
    "    timevar = data.columns[0]\n",
    "    data[timevar] = data[timevar].astype('datetime64[D]')\n",
    "    full_time_range = pd.date_range(data[timevar].min(),  \n",
    "                                            data[timevar].max(), \n",
    "                                            freq = timefreq)\n",
    "    data = data.set_index(timevar)\n",
    "    data = data.reindex(full_time_range,  fill_value=0)\n",
    "    data.index.name = 'date'\n",
    "    return(data)\n",
    "\n",
    "def day_lag(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by region and date\n",
    "    df = df.sort_values(['date'])\n",
    "    \n",
    "    # Lag value\n",
    "    df['value_l'] = df['value'].shift(1)\n",
    "    \n",
    "    # Drop values if missing dates\n",
    "    df['value_l'] = df['value_l'].where(df.date.diff() == dt.timedelta(days = 1), np.nan)\n",
    "    \n",
    "    return df['value_l']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SAq88YZLwB"
   },
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCRBfqMGLjq7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K86xN61OmPU"
   },
   "source": [
    "First, let's import the datasets we will use on this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq68weiSOV-X"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0FPdBvpOZQQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load and have a look at each one of the indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqCoflgQZRSY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load transactions per day data\n",
    "trans = pd.read_csv('transactions_per_day.csv')\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load subscribers per day data\n",
    "subs = pd.read_csv('subscribers_per_day.csv')\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movements\n",
    "mov = pd.read_csv('movements_per_day.csv')\n",
    "mov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgBRPdR0L_yK"
   },
   "source": [
    "## Exercise 1 - Quality checks\n",
    "\n",
    "First step is to take a quick look at the completeness and consistency of the data.\n",
    "\n",
    "- Check the number of regions per day\n",
    "- Check time completeness of the series\n",
    "- Compare number of subscribers to the number of calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 instructions:\n",
    "1. Aggregate the transactions data, `trans`, by date. Store the result in a new DataFrame called `trans_day`\n",
    "    - Calculate the number of unique regions per day (TIP: use the `pd.Series.nunique` function.\n",
    "    - Calculate sum of total transactions per day\n",
    "2. Test if any date has fewer than 16 regions\n",
    "3. Run the time complete function (user defined) to create a new DataFrame replacing missing rows with zeros:\n",
    "    `trans_day_tcomplete = time_complete(trans_day)`\n",
    "4. Plot the `value` column of the time complete data frame.\n",
    "\n",
    "Do you see anything unusual?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPfN_10taird"
   },
   "outputs": [],
   "source": [
    "# 1. Aggregate data by day summing values across all regions\n",
    "trans_day = trans.groupby('date').agg({'pcod': pd.Series.nunique,\n",
    "                                       'value': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  Test if any date has fewer than 16 regions\n",
    "type(trans_day[trans_day['pcod'] < 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGI6n3rzaird",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLot\n",
    "trans_day = time_complete(trans_day)\n",
    "trans_day.plot(y = 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 2 instructions:\n",
    "1. Merge `subs` and `trans` using `['region', 'date']` columns as keys.\n",
    "\n",
    "    TIP: use the `suffixes` argument to differentiate the values on from each DataFrame \n",
    "\n",
    "2. Use a scatterplot to compare the values of the two columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs\\\n",
    "    .merge(trans, \n",
    "           on = ['pcod', 'date'], \n",
    "           suffixes = ('_trans', '_subs'))\\\n",
    "    .plot.scatter(x = 'value_subs',\n",
    "                 y = 'value_trans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVWvRSW0MHdY"
   },
   "source": [
    "## Exercise 2 - Changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSUj4uo9iCqT"
   },
   "source": [
    "Now let's look how movement has changed over time. \n",
    "\n",
    "For simplicity we will use country level data and only look at movements between two different regions. Here's a quick summary of the comparisons we'll do:\n",
    "\n",
    "- Absolute values\n",
    "- Change from previous day\n",
    "- Change from Baseline (defined as the average from February 1st to March 15th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here is a time-line to help interpret the results:\n",
    "- February 1st to March 15th: Baseline period\n",
    "- March 16th: initial restrictions imposed\n",
    "- March 30th: Lockdown measures on parts of Accra and Kumasi metropolitan areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. On the `mov` DataFrame, remove rows where users move within regions, that is `pcod_to` and `pcod_from` are the same\n",
    "2. Aggregate `mov` DataFrame by `date` to have country level data\n",
    "3. Use the `day_lag()` function to create a DataFrame with a column containing the value of movements from the previous day.\n",
    "4. Use the `bl_values()` function to create a DataFrame with a column containing the average number of movements in the baseline.\n",
    "5. Create percent changes from previous day and from baseline columns.\n",
    "6. Create 3 different line plots:\n",
    "    - Level values of total movements\n",
    "    - Percent change from previous day\n",
    "    - Percent change from baseline\n",
    "\n",
    "How does these 3 plots compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move to part 4, lets review what the `bl_value()` function is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl_values(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create weekday variable to calculate baseline values\n",
    "    df['weekday'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Keep only entries from Feb 1st to Mar 15th\n",
    "    bl = df[df['date'] < dt.datetime(2020, 3, 16)]\n",
    "\n",
    "    # Calculate baseline averages for each weekday\n",
    "    # bl_averages = bl.groupby(['pcod', 'weekday']).agg({'value': np.mean}).reset_index()\n",
    "    bl_averages = bl.groupby(['weekday']).agg({'value': np.mean}).reset_index()\n",
    "    \n",
    "    # Merge bl averages as a column on original df\n",
    "    # ndf = df.merge(bl_averages, on = ['pcod', 'weekday'],\n",
    "    #                suffixes = ('', '_bl')).drop('weekday', axis = 1)\n",
    "    ndf = df.merge(bl_averages, on = ['weekday'],\n",
    "                  suffixes = ('', '_bl')).drop('weekday', axis = 1)\n",
    "\n",
    "    return ndf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove movements in the same district\n",
    "mov = mov[mov['pcod_from'] != mov['pcod_to']]\n",
    "\n",
    "# 2. Aggregate by day\n",
    "mov_day = mov.groupby('date').agg({'value' : np.sum}).reset_index()\n",
    "\n",
    "# 3\n",
    "mov_day = day_lag(mov_day)\n",
    "\n",
    "# 4\n",
    "mov_day = bl_values(mov_day)\n",
    "\n",
    "# 5. Calculate percent change columns\n",
    "mov_day['p_change_l'] = (mov_day['value'] - mov_day['value_l'])/mov_day['value_l']\n",
    "\n",
    "mov_day['p_change_bl'] = (mov_day['value'] - mov_day['value_bl'])/mov_day['value_bl']\n",
    "\n",
    "# mov_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_day_lag.plot(x = 'date', y = 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_day_lag.plot(x = 'date', y = 'p_change_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movs_day_bl.plot(x = 'date', y = 'p_change_bl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yDEra3bMHoG"
   },
   "source": [
    "\n",
    "## Exercise 3 - Choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI954J60amL3"
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"admin1.geojson\")\n",
    "a1_geo = r'admin1.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs_day = subs_day.reset_index()\n",
    "subs['month'] = subs['date'].dt.month\n",
    "subs_month = subs.groupby(['month', 'pcod']).agg({'value': np.sum}).reset_index()\n",
    "cho_data4 = subs_month[subs_month['month'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "m = folium.Map(location=[7.28, -0.97], zoom_start=7)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=a1_geo,\n",
    "    name=\"May\",\n",
    "    data=cho_data5,\n",
    "    columns=[\"pcod\", \"value\"],\n",
    "    key_on=\"feature.properties.pcod\",\n",
    "    fill_color=\"BuPu\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "#     highlight = True,\n",
    "#     reset = True,\n",
    "#     overlay = False\n",
    ").add_to(m)\n",
    "\n",
    "# Add control to the side \n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": " aggregated-cdr-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
