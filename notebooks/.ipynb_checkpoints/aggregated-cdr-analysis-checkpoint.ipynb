{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LeonardoViotti/cdr-training/blob/develop/notebooks/aggregated-cdr-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKd3pER2LVMu"
   },
   "source": [
    "# CDR Analysis\n",
    "\n",
    " - Aggregated data\n",
    " - Quality Checks\n",
    " - Analysis\n",
    " - Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsOgtQfYalb"
   },
   "source": [
    "# Environment set-up\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VOVB-FHL0yL"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Libraries installation\n",
    "\n",
    "# !pip install geopandas\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Useful functions\n",
    "\n",
    "def time_complete(data, timefreq = 'D'):\n",
    "    data = data.reset_index()\n",
    "    timevar = data.columns[0]\n",
    "    data[timevar] = data[timevar].astype('datetime64[D]')\n",
    "    full_time_range = pd.date_range(data[timevar].min(),  \n",
    "                                            data[timevar].max(), \n",
    "                                            freq = timefreq)\n",
    "    data = data.set_index(timevar)\n",
    "    data = data.reindex(full_time_range,  fill_value=0)\n",
    "    data.index.name = 'date'\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SAq88YZLwB"
   },
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCRBfqMGLjq7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K86xN61OmPU"
   },
   "source": [
    "First, let's import the datasets we will use on this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq68weiSOV-X"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0FPdBvpOZQQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load and have a look at each one of the indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqCoflgQZRSY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load transactions per day data\n",
    "trans = pd.read_csv('transactions_per_day.csv')\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load subscribers per day data\n",
    "subs = pd.read_csv('subscribers_per_day.csv')\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgBRPdR0L_yK"
   },
   "source": [
    "## Exercise 1 - Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPfN_10taird"
   },
   "outputs": [],
   "source": [
    "# Aggregate data by day summing values across all regions\n",
    "subs_day = subs.groupby('date').agg({'value': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGI6n3rzaird",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLot\n",
    "subs_day = time_complete(subs_day)\n",
    "subs_day.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVWvRSW0MHdY"
   },
   "source": [
    "## Exercise 2 - Changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSUj4uo9iCqT"
   },
   "source": [
    "On the orinal data (region and day level) create percent change columns:\n",
    "- Change from previous day\n",
    "- Change from Baseline (defined as the average for each region in February)\n",
    "\n",
    "Plot day level data just like the previous exercisse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeline:\n",
    "- February 1st to March 15th: Baseline period\n",
    "- March 16th: initial restrictions imposed\n",
    "- March 30th: Lockdown measures on parts of Accra and Kumasi metropolitan areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag variable\n",
    "# Absolute change from previous day\n",
    "# % change from previous day\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_lag(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by region and date\n",
    "    df = df.sort_values(['pcod', 'date'])\n",
    "    \n",
    "    # Lag value\n",
    "    df['value_l'] = df.groupby('pcod')['value'].shift(1)\n",
    "    \n",
    "    # Drop values if missing dates\n",
    "    df['value_l'] = df['value_l'].where(df.groupby('pcod').date.diff() == dt.timedelta(days = 1), np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "day_lag(subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghanna lockdown?\n",
    "# Crate baseline average Feb values\n",
    "# % change from baseline\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl_values(df):\n",
    "    # Makse sure date is datetime type\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create weekday variable to calculate baseline values\n",
    "    df['weekday'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Keep only entries from Feb 1st to Mar 15th\n",
    "    bl = df[df['date'] < dt.datetime(2020, 3, 16)]\n",
    "\n",
    "    # Calculate baseline averages for each weekday\n",
    "    # bl_averages = bl.groupby(['pcod', 'weekday']).agg({'value': np.mean}).reset_index()\n",
    "    bl_averages = bl.groupby(['weekday']).agg({'value': np.mean}).reset_index()\n",
    "    \n",
    "    # Merge bl averages as a column on original df\n",
    "    # ndf = df.merge(bl_averages, on = ['pcod', 'weekday'],\n",
    "    #                suffixes = ('', '_bl')).drop('weekday', axis = 1)\n",
    "    ndf = df.merge(bl_averages, on = ['weekday'],\n",
    "                  suffixes = ('', '_bl')).drop('weekday', axis = 1)\n",
    "\n",
    "    return ndf\n",
    "\n",
    "subs_day_bl = bl_values(subs_day.reset_index())\n",
    "\n",
    "# subs_day.reset_index()\n",
    "# subs = bl_values(subs)\n",
    "# subs_day.reset_index()\n",
    "# subs_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_day_bl['p_change_bl'] = (subs_day_bl['value'] - subs_day_bl['value_bl'])/subs_day_bl['value_bl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_day_bl.plot(x = 'date', y = 'p_change_bl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yDEra3bMHoG"
   },
   "source": [
    "\n",
    "## Exercise 3 - Choropleth\n",
    "\n",
    "TO DO:\n",
    "- Convert to admin \n",
    "- Make sure column names match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI954J60amL3"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"admin1.geojson\")\n",
    "a1_geo = r'admin1.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tdf_region = tdf.groupby('region').agg({'value': np.sum}).reset_index()\n",
    "# tdf_region = tdf_region.rename(columns = {'region': 'pcod'})\n",
    "foo = pd.read_csv('location_event_counts_admin1_2020-02-05.csv')\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = (\n",
    "#     \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data\"\n",
    "# )\n",
    "# state_unemployment = f\"{url}/US_Unemployment_Oct2012.csv\"\n",
    "# state_data = pd.read_csv(state_unemployment)\n",
    "# state_geo = f\"{url}/us-states.json\"\n",
    "# state_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "m = folium.Map(location=[7.28, -0.97], zoom_start=7)\n",
    "\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=a1_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=foo,\n",
    "    columns=[\"pcod\", \"value\"],\n",
    "    key_on=\"feature.properties.pcod\",\n",
    "    fill_color=\"BuPu\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    highlight = True,\n",
    "    reset = True,\n",
    ").add_to(m)\n",
    "\n",
    "# Add control to the side \n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": " aggregated-cdr-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
